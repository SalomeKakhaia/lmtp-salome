\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after obs_study_style.
% Note that obs_study_style.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which

\RequirePackage{graphicx,color,ae,fancyvrb}
\RequirePackage[T1]{fontenc}

\usepackage{obs_study_style}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{sansmath}

% Definitions of handy macros can go here
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\Q}{\mathsf{m}}
\renewcommand{\r}{\mathsf{r}}
\newcommand{\g}{\mathsf{g}}
\newcommand{\p}{\mathsf{p}}
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\E}{\mathsf{E}} 
\newcommand{\VAR}{\mathsf{VAR}}
\newcommand{\COV}{\mathsf{COV}}
\newcommand{\Prob}{\mathsf{P}}
\let\code=\texttt

% For papers submitted for review, just fill in author names
% For accepted papers, heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\heading{}{}{}{}{}{Nicholas Williams and Iván Díaz}

% Short headings should be running head and authors last names
\ShortHeadings{Estimating the Causal Effects of Modified Treatment Policies in R}{Williams and Díaz}
\firstpageno{1}

\begin{document}

\title{lmtp: An R Package for Estimating the Causal Effects of Modified Treatment Policies}

\author{\name Nicholas Williams \email ntw2117@cumc.columbia.edu \\
       \addr Department of Epidemiology \\
       Mailman School of Public Health \\
       Columbia University \\
       New York, NY 10032, USA
       \AND
       \name Iván Díaz \email ild2005@med.cornell.edu \\
       \addr Division of Biostatistics and Epidemiology \\
       Department of Population Health Sciences \\
       Weill Cornell Medicine \\
       New York, NY 10065, USA}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
The majority of causal inference methods consider treatment effects based on counterfactual outcomes where exposure is deterministically assigned. When exposure is continuous, deterministic interventions may be irrelevant and impossible to bring about. As a solution, modified treatment policies offer a generalization that allows for the study of feasible interventions and offer a safeguard against positivity violations. The \pkg{lmtp} package implements the estimators of \citet{jasaLMTP} for estimating causal effects based on non-parametric modified treatment policies in \texttt{R}. In addition to modified treatment policies, the package can be used to estimate effects of deterministic and dynamic interventions. The methods provided can also be applied to both point-treatment and longitudinal settings, and can account for time-varying exposure, covariates, and right censoring, thereby providing a very general tool for causal inference in longitudinal studies. Additionally, two of the provided estimators are based on flexible machine learning regression algorithms, and avoid bias due to parametric model misspecification while maintaining valid statistical inference.
\end{abstract}

\begin{keywords}
Causal inference, longitudinal data, non-parametric, modified treatment policies
\end{keywords}

\section{Introduction}

Most modern causal inference methods consider the effects of a treatment on a population mean outcome under interventions that set the treatment value deterministically. For example, the average treatment effect (ATE) considers the hypothetical difference in a population mean outcome if a dichotomous exposure was applied to all observations versus if it was applied to none. In the case of a continuous exposure, interventions that set the exposure to a static value deterministically are of little practical relevance. Furthermore, the estimation of causal effects requires the so called positivity assumption which states that all observations have a greater than zero chance of experiencing the exposure value under consideration \citep{rosenbaumCentralRolePropensity1983}. This assumption is often violated when evaluating the effects of deterministic interventions, and is usually exacerbated with longitudinal data as the number of time points grows.

Modified treatment policies (MTPs) are a class of stochastic treatment regimes that can be formulated to avoid the above problems \citep{munozPopulationInterventionCausal2012, haneuseEstimationEffectInterventions2013}. In a recent article
\citep{jasaLMTP}, we generalized the theoretical framework for estimation of the effect of MTPs to the longitudinal setting, accounting for time-varying treatment, covariates, and right-censoring of the outcome. Briefly, MTPs are hypothetical interventions where the post-intervention value of treatment can depend on the actual observed treatment level and the
unit's history. As such, MTPs are useful to assess the effect of continuous treatments. For example, \citet{haneuseEstimationEffectInterventions2013} assess the effect of reducing surgery time by a predetermined amount (e.g., 5 minutes) for lung cancer patients, where the reduction is carried out only for those patients for whom the intervention is feasible. Furthermore, MTPs generalize many important effect estimands, such as the effect of a dynamic treatment rule in which the treatment level is assigned as a function of a unit's history. For example, dynamic treatment rules, a particular case of MTPs, may be used to estimate the effect of policies such as switching HIV treatment once the CD4 T-cell count goes below a predetermined threshold \citep{petersen2014delayed}. MTPs also generalize many interesting causal effects such as the average treatment effects, the causal risk ratio, and causal odds ratio. In this article we describe how \pkg{lmtp} can be used for estimating the causal effects of MTPs, and present examples on the use of the software for several of the above cases.

The package lmtp implements four methods for estimating the effects of MTPs. Two of these estimators, a targeted minimum-loss based estimator (TMLE) \citep{laanTargetedLearningCausal2011a, laanTargetedMaximumLikelihood2006} and a sequentially doubly-robust estimator (SDR) \citep{buckleyLinearRegressionCensored1979, fanCensoredRegressionLocal1994, vanderlaanUnifiedCrossValidationMethodology2003, rotnitzkyDoublyRobustEstimation2006, rubinDoublyRobustCensoring2006, kennedyNonparametricMethodsDoubly2017}, are multiply-robust. TMLE and SDR are implemented using cross-fitting to allow for the use of flexible machine learning regression methodology \citep{jasaLMTP}. The package may be download from CRAN at \url{cran.r-project.org/package=lmtp}.

\hypertarget{notation-and-modified-treatment-policies}{%
\section{Notation and modified treatment policies}\label{notation-and-modified-treatment-policies}}

\hypertarget{data-structure}{\subsection{Data structure}\label{data-structure}}

We will use the notation of \citet{jasaLMTP} with only slight modifications.  Let $i$ be the index of an observation from a data set with $n$ total units and $t$ be the index of time for a total number of time points $\tau$.  The observed data for observation $i$ may be denoted as

\begin{equation}
Z_i = (W, L_1, A_1, L_2, A_2, ..., L_{\tau}, A_{\tau}, Y_{\tau + 1})
\end{equation}

where $W$ denotes baseline covariates, $L_t$ denotes time-varying covariates, $A_t$ denotes a vector of exposure and/or censoring variables and $Y$ denotes an outcome measured at the end of study follow-up. We observe $n$ i.i.d. copies of $Z$ with distribution $\Prob$. We use $A_t = a_t$ to denote a realization of a random variable. If right-censoring exists, $A_t$ can be adapted so that $A_t = (A_{1, t}, A_{2, t})$ where $A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denotes the exposure at time $t$. We use an overbar to indicate the history of a variable up until time $t$. We then use $H_t = (\bar{L}_t, \bar{A}_{t-1})$ to denote the history of all variables up until just before $A_t$.

\hypertarget{modified-treatment-policies}{\subsection{Modified treatment policies}\label{modified-treatment-policies}}

We use the potential outcomes framework to define the causal effect of interest using our established data structure. We consider a hypothetical policy where $\bar{A}$ is set to a regime $d$ defined as $A^{d}_t = d_t(A_t, H^{d}_t)$, where $H^{d}_t = (\bar{L}_t, \bar{A}^{d}_{t - 1})$, for a set of user-given regimes $d_t:t \in \{1, ..., \tau\}$. The defining characteristic that makes regime $d_t$ a modified treatment policy is that it depends on the \emph{natural value} of treatment $\bar{A}_t$, that is, the value that the treatment would have taken under no intervention. However, when the function $d_t$ only depends on $H_t$, the LMTP reduces to the \textit{dynamic treatment regimes} studied in the literature. Furthermore, when $d_t$ is a constant that and does not depend on either $A_t$ or $H_t$, then LMTPs reduce to the conventional static rules studied in the causal inference literature \citep[e.g.,][]{bang2005doubly, van2011targeted}. Below we present examples of all these interventions.

First, consider a study of the effect of physical activity on mortality in the elderly. Assume that each patient is monitored at several time points, and that a measure of physical activity such as the metabolic equivalent of task (MET) \citep{mendes2018metabolic} is measured together with a number of lifestyle, health status, and demographic variables. In this setup, a natural question to ask would be ``what is the effect on mortality of an intervention that increases physical activity by $\delta$ units for patients whose socioeconomic and health status allows it?'' Formally, consider a longitudinal study with loss-to-follow-up. Let $A_t = (A_{1, t}, A_{2, t})$ where $A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denote a continuous exposure at time $t$ that can be changed through some intervention. A modified treatment policy that increases $A_{2,t}$, whenever it is feasible to do so, can be defined as

\begin{equation}\label{eq:mtp}
d_t(a_t,h_t)=
\begin{cases}
  (1, a_{2,t} + \delta_t) & \text{if } a_{2,t} + \delta_t \leq u_t(h_t)  \\
  (1, a_{2,t}) & \text{if } a_{2,t} + \delta_t > u_t(h_t)
\end{cases}
\end{equation}

where $u_t(h_t)$ defines the maximum level of physical activity allowed for a patient with characteristics $h_t$. Note that we also consider an intervention on $A_{1,t}$ because we are interested in a hypothetical world where there is no loss-to-follow-up. In this case the hypothetical exposure after intervention, $A^{d}_t$ depends on the actually observed exposure, $A_t$. This is in contrast to a deterministic intervention where $A^{d}_t$ would be set to some pre-specified value with probability one.

For dynamic treatment rules, consider a hypothetical longitudinal study where two different antiviral treatments are administered to HIV positive patients. Sometimes an antiviral drug works at first, until the virus develops resistance, at which point it is necessary to change the treatment regime. Assume we are interested in assessing a policy with two treatments encoded as $A_t\in \{0,1\}$, and we want to assess the effect of a regime that would switch the antiviral treatment as soon as the CD4 T cell count drops bellow 300. Let $A_t = (A_{1, t}, A_{2, t})$ where $A_{1, t}$ equals one if an observation is still in the study at time $t$ and zero otherwise, and $A_{2, t}$ denotes the treatment arm at time $t$. Let $L_t$ denote the CD4 T cell count at time $t$. In this case, one may decide to assess the effect of the rule

\begin{equation}\label{eq:dyn}
  d_t(h_t)=
  \begin{cases}
    (1, 1 - a_{2,t-1}) & \text{if } l_t < 300  \\
    (1, a_{2,t-1}) & \text{if } l_t  \geq 300.
  \end{cases}
\end{equation}

In contrast to the previous rule (\ref{eq:mtp}), the dynamic treatment rule (\ref{eq:dyn}) does not depend on the natural value of treatment at time $t$, it only depends on the history. The software and methods presented here handle both cases seamlessly.

In the case of a single time point setting where the data structure is $Z=(W,A,Y)$, it follows trivially from the above definitions that the average treatment effect from a cross-sectional study, defined as $\E[Y(1) - Y(0)]$, can be estimated using MTPs by simply letting $\tau = 1$ and contrasting two MTPs $d(A)=1$ and $d(A)=0$. The \pkg{lmtp} package presented in this article allows the contrast of different MTPs using differences, ratios, and odds ratios. We provide examples of these contrasts in \S \ref{examples} below.

In what follows we focus on estimating the the causal effect of MTP $d$ on outcome $Y$, using \pkg{lmtp}, through the causal parameter

\begin{equation}
  \theta = \E\{Y(\bar A^d)\}\text{,}
\end{equation}

where $Y(\bar A^d)$ is the counterfactual outcome in a world, where possibly contrary to fact, each entry of $\bar{A}$ was modified according to the MTP $d$. When $Y$ is continuous, $\theta$ is the mean population value of $Y$ under MTP $d$; when $Y$ is dichotomous, $\theta$ is the population proportion of event $Y$ under MTP $d$. Similarly, when $Y$ is the indicator of an event by end of the study, $\theta$ is defined as the cumulative incidence of $Y$ under MTP $d$.

\hypertarget{identification}{\subsection{Identification}\label{identification}}

The ability to estimate $\theta$ depends on the ability to identify an expression for $\theta$ as a function of the data generating distribution $\Prob$ using only the observed data $Z$ and not counterfactual variables $Y^d$. A full review of these identification assumptions is outside the scope of this article but is presented in our technical paper \citep{jasaLMTP}. Briefly, the following standard assumptions must hold

\newtheorem{assumption}{Assumption}

\begin{assumption}[Consistency]\label{ass:cons}
$\bar{A} = \bar{a} \implies Y = Y(\bar{a})$ for all $\bar{a} \in \mathop{\mathrm{supp}}\bar{A}$ 
\end{assumption}
\begin{assumption}[Exchangeability]\label{ass:ex}
  $A_t \perp \!\!\! \perp Y(\bar{a}) | H_t$ for all
  $\bar{a} \in \mathop{\mathrm{supp}}\bar{A}$ and
  $t \in \{1, ..., \tau\}$
\end{assumption}
\begin{assumption}[Positivity]\label{ass:pos}
  If $(a_t, h_t) \in \mathop{\mathrm{supp}}\{A_t, H_t\}$ then
  $(d(a_t, h_t), h_t) \in \mathop{\mathrm{supp}}\{A_t, H_t\}$ for
  $t \in \{1, ..., \tau \}$
\end{assumption}

The consistency assumption states that the potential outcome for an observation under their observed exposure is the value of the outcome that we did actually observe. The exchangeability assumption is often also referred to as the no-unmeasured confounding assumption; it is satisfied if all common causes of the exposure and outcome are measured and adjusted for. Of particular importance to this article is the positivity assumption which states that the distribution of the exposure under the MTP is supported in the data. Concretely, in a study with a continuous exposure and loss-to-follow-up, the positivity assumption states that if an observation with covariate history $h_t$ and exposure $a_t$ who was not lost-to-follow-up at time $t$ exists then there is also an observation with covariate history $h_t$ who was not lost-to-follow-up at time $t$ but whose exposure was observed as $d(a_t, h_t)$ that also exists. This assumption is often an issue when working with continuous exposures and/or multiple time points in the context of static or dynamic interventions. One strength of MTPs is that if the support of the exposure variable is known, then they may be formulated to avoid violations of the positivity assumption.

Under these identification assumptions, the causal parameter $\theta$ is identified as follows. Set $\Q_{\tau+1}= Y$. For $t=\tau,\ldots,1$, recursively define

\begin{equation}
  \Q_t:(a_t, h_t) \mapsto \E\left[\Q_{t+1}(A_{t+1}^d, H_{t+1})\mid
    A_t=a_t, H_t=h_t\right].\label{eq:defQ}
\end{equation} 

Then $\theta$ is identified as $\E[\Q_1(A_1^d, L_1)]$. In addition to the function $\Q_t$, the description of the estimation methods below will benefit from the definition of the ratio between the density of the post-intervention exposure, $A_t^d$, and the density of the observed exposure, $A_t$. That is, define $\g_t^d(a_t \mid h_t)$ as the density of $A_t^d$ conditional on $H_t=h_t$ evaluated at $h_t$. Likewise define $\g_t(a_t\mid h_t)$ as the conditional density of $A_t$. Our estimation procedure will make use of the density ratio defined as: \[\r_t(a_t, h_t) = \frac{\g_t^d(a_t \mid h_t)}{\g_t(a_t \mid h_t)}.\] This density ratio reduces to the well-known inverse probability weights used in estimation of the ATE in the single-time point setting where $Z=(W,A,Y)$ and $A$ is binary.

\hypertarget{estimating-modified-treatment-policy-effects}{\section{Estimating modified treatment policy effects}\label{estimating-modified-treatment-policy-effects}}

\hypertarget{estimation-methods}{\subsection{Estimation methods}\label{estimation-methods}}

The \pkg{lmtp} package implements four estimation methods: a targeted minimum-loss based estimator (TMLE), a sequential doubly-robust estimator (SDR), an estimator based on the parametric G-formula, and an inverse probability weighted (IPW) estimator. We will only describe the use of TMLE, \code{lmtp\_tmle}, and SDR, \code{lmtp\_sdr}, as their use is strongly suggested over the others based on their advantageous theoretical properties which allow for machine learning regression while maintaining the ability to compute valid confidence intervals and p-values.

Targeted minimum-loss based estimation is a general framework for constructing asymptotically linear estimators leveraging machine learning, with an optimal bias-variance tradeoff for the target causal parameter \citep{vanderLaanRose11, vanderLaanRose18}. In general, TMLE is constructed from a factorization of observed data likelihood into an outcome regression and an intervention mechanism. Using the outcome regression, an initial estimate of the target parameter is constructed and then \textit{de-biased} by a fluctuation that depends on a function of the intervention mechanism. The sequential doubly-robust estimator is based on a unbiased transformation of the efficient influence function of the target estimand. For a thorough discussion of TMLE and SDR for static, dynamic, and modified treatment policies, we refer the reader to \cite{van2011targeted,luedtke2017sequential,rotnitzky2017multiply,jasaLMTP}.

TMLE and SDR require estimation of two nuisance parameters at each time point: an outcome mechanism and an intervention mechanism. Both TMLE and SDR are multiply-robust in that they allow certain configurations of nuisance parameters to be inconsistently estimated. Specifically, TMLE is considered $\tau + 1$-multiply robust in that it allows for inconsistent estimation of all the intervention mechanisms prior to any time point $t$, as long as all outcome mechanisms after time $t$ are consistently estimated. SDR is $2^{\tau}$-robust in that at each time point, estimation of at most either the intervention mechanism or outcome mechanism is allowed to be inconsistent. Both TMLE and SDR are efficient when all the treatment mechanism and outcome regression are consistently estimated at a given consistency rate, but the SDR has better protection against model misspecification \citep[see][for more details]{luedtke2017sequential,rotnitzky2017multiply, jasaLMTP}.

It is important to note that the SDR estimator can produce an estimate $\hat{\theta}$ outside of the bounds of the parameter space (e.g., probability estimates outside $[0,1]$), while the TMLE guarantees that the estimate is within bounds of the parameter space. With this in mind and because for a single time-point TMLE and SDR are equally robust, we recommend use of TMLE for the case of a single time-point, while we recommend use of SDR for the longitudinal setting.

\hypertarget{required-data-structure}{\subsection{Required data structure}\label{required-data-structure}}

Data is passed to \pkg{lmtp} estimators through the \code{data} argument. Data should be in wide format with one column per variable per time point under study (i.e., there should be one column for every variable in $Z$). These columns do not have to be in any specific order and the data set may contain variables that are not used in estimation. The names of treatment variables, censoring variables, baseline covariates, and time-varying covariates are specified using the \code{trt}, \code{cens}, \code{baseline}, and \code{time\_vary} arguments respectively. The \code{trt}, \code{cens}, and \code{baseline} arguments accept character vectors and the \code{trt} and \code{cens} arguments should be ordered according to the time-ordering of the data generating mechanism. The \code{time\_vary} argument accepts an unnamed list sorted according to the time-ordering of the model with each index containing the name of the time-varying covariates for the given time. The outcome variable is specified through the \code{outcome} argument.

Estimators are compatible with continuous, dichotomous and survival outcomes. In the case of a dichotomous or continuous outcome, only a single variable name should be passed to the \code{outcome} argument. For survival outcomes, a vector containing the names of the intermediate outcome and final outcome variables, ordered according to time, should be specified with the \code{outcome} argument. Dichotomous and survival outcomes should be coded using zero's and one's where one indicates the occurrence of an event and zero otherwise. If working with a survival outcome, once an observation experiences an outcome, all future outcome variables should also be coded with a one. The \code{outcome\_type} argument should be set to \code{"continuous"} for continuous outcomes, \code{"binomial"} for dichotomous outcomes, and \code{"survival"} for time-to-event outcomes. If the study is subject to loss-to-follow-up, the \code{cens} argument must be provided. Censoring indicators should be coded using zero's and one's where one indicates an observation is observed at the next time and zero indicates loss-to-follow-up. Once an observation's censoring status is switched to zero it cannot change back to one. Missing data before an observation is lost-to-follow-up is not allowed; a preprocessing step using multiple imputation is recommended for such variables.

The \code{k} argument controls a Markov assumption on the data generating mechanism. When \code{k = Inf}, the history $H_t$ will be constructed using all previous time-point variables while setting \code{k} to any other value will restrict $H_t$ to time-varying covariates from time $t - k - 1$ until $t-1$. Baseline confounders are always included in $H_t$. The
\code{create\_node\_list} function may be used to inspect how variables will be used for estimation. It is specified with the same \code{trt}, \code{baseline}, \code{time\_vary}, and \code{k} arguments as \pkg{lmtp} estimators and is used internally to create a ``node list'' that encodes which variables should be used at each time point of estimation. For example, consider a study with the observed data structure

\begin{equation}
Z = (W_1, W_2, L_{1, 1}, L_{1, 2}, A_1, L_{2, 1}, L_{2, 2}, A_2, Y_3)
\end{equation}

We can translate this data structure to \proglang{R} with

\begin{CodeChunk}

\begin{CodeInput}
R> W <- c("W_1", "W_2")
R> A <- c("A_1", "A_2")
R> L <- list(c("L_11", "L_12"), c("L_21", "L_22"))
R> create_node_list(trt = A, baseline = W, time_vary = L, tau = 2)
\end{CodeInput}

\begin{CodeOutput}
$trt
$trt[[1]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1" 

$trt[[2]]
[1] "W_1"  "W_2"  "L_11" "L_12" "L_21" "L_22" "A_1"  "A_2" 


$outcome
$outcome[[1]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1" 

$outcome[[2]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1"  "L_21" "L_22" "A_2" 
\end{CodeOutput}
\end{CodeChunk}

A list of lists is returned with the names of the variables in $H_t$ to be used for estimation of the outcome regression and the treatment mechanism at every time $t$.  Notice that variables $A_1$ and $A_2$ are included in the list of variables used for estimation of the treatment mechamism (\code{trt}). This is due to the fact that the nuisance parameter for the treatment mechanism is the density ratio $\r_t$, which is a function of $A_1$ and $A_2$.

The density ratio is estimated based on a classification trick using an auxiliary variable $\Lambda$ as a pseudo outcome and the treatment as a predictor. We now briefly describe how this density ratio estimation is done; the process is fully automated and hidden from the software user. Specifically, the TMLE and SDR estimation methods require estimation of the ratio of the densities of $A_t^d$ and $A_t$, conditional on the history $H_t$, defined as $\r_t$ above. This is achieved through computing the odds in a classification problem in an augmented dataset with $2n$ observations where the outcome is the auxiliary variable $\Lambda$ (defined below) and the predictors are the variables $A_t$ and $H_t$. In the $2n$ augmented data set, the data structure at time $t$ is redefined as

\begin{equation}
(H_{\lambda, i, t}, A_{\lambda, i, t}, \Lambda_{\lambda, i} : \lambda = 0, 1; i = 1, ..., n)
\end{equation}

where $\Lambda_{\lambda, i} = \lambda_i$ indexes duplicate values. For all duplicated observations $\lambda\in\{0,1\}$ with the same $i$, $H_{\lambda, i, t}$ is the same. For $\lambda = 0$, $A_{\lambda, i, t}$ equals the observed exposure values $A_{i, t}$, whereas for $\lambda=1$, $A_{\lambda, i, t}$ equals the exposure values under the MTP $d$, namely $A^{d}_t$. The classification approach to density ratio estimation proceeds by estimating the conditional probability that $\Delta=1$ in this dataset, and dividing it by the corresponding estimate of the conditional probability that $\Delta=0$. Specifically, denoting $\p^\lambda$ the distribution of the data in the augmented dataset, we have:

\[\r_t(a_t, h_t) = \frac{\p^\lambda(a_t, h_t \mid \Lambda =
    1)}{\p^\lambda(a_t, h_t \mid \Lambda =
    0)}=\frac{\p^\lambda(\Lambda = 1\mid A_t=a_t,
    H_t=h_t)}{\p^\lambda(\Lambda = 0\mid A_t=a_t, H_t=h_t)}.\] 
    
Further details on this algorithm may be found in our technical paper \citep{jasaLMTP}.

\hypertarget{shift-functions}{\subsection{Specifying Interventions}\label{shift-functions}}

The treatment rule $d_t$ can be specified using one of two arguments: \code{shift} or \code{shifted}. If using \code{shift}, the analyst provides a two-argument function where the first argument should correspond to a dataset containing the variables in $Z$ and the second argument should accept a string corresponding to the variable $A_t$ in $Z$. This function should return a size $n$ vector of $A_t$ modified according to $d_t$. We provide examples of these functions in \S \ref{examples} below. Alternatively, the analyst can supply a modified version of \code{data} to \code{shifted} where $A_t$ are replaced with $A^d_t$.

\hypertarget{machine-learning}{\subsection{Machine Learnning}\label{machine-learning}}

An attractive property of multiply-robust estimators is that they can incorporate flexible machine-learning algorithms for the estimation of nuisance parameters $\Q_t$ and $\r_t$ while remaining $\sqrt{n}$-consistent. The super learner algorithm is an ensemble learner than incorporates a set of candidate models through a weighted convex-combination based on cross-validation \citep{laanSuperLearner2007}. Asymptotically, this weighted combination of models, called the meta-learner, will outperform any single one of its components.

Our package uses the implementation of the super learner provided by the \pkg{SuperLearner} package \citep{SuperLearnerPkg}. The algorithms to be used in the super learner with \code{lmtp\_tmle} and \code{lmtp\_sdr} calls are specified with the \code{lrnrs\_trt} and \code{lrnrs\_outcome} arguments. The outcome variable type should guide users on selecting the appropriate candidate learners for use with the \code{lrnrs\_outcome} argument. Regardless of whether an exposure is continuous, dichotomous, or categorical, the exposure mechanism is estimated using classification as discussed above, users should thus only include candidate learners capable of binary classification with the \code{lrnrs\_trt} argument.

Candidate learners that rely on cross-validation for the tuning of hyper-parameters should support grouped data if used with \code{lrnrs\_trt}. Because estimation of the treatment mechanism relies on the augmented $2n$ duplicated data set, duplicated observations must be put into the same fold during sample-splitting. This is done automatically by the package.

\hypertarget{additional-arguments}{\subsection{Additional arguments}\label{additional-arguments}}

Sample-splitting and cross-fitting is used with all methods to avoid certain technical conditions that may not hold for machine learning estimators \citep{zhengCrossValidatedTargetedMinimumLossBased2011b,chernozhukovDoubleDebiasedMachine2018}. Specifically, the data are split in $V$ volds and each nuisance parameter is estimated $V$ times, excluding data in one fold each time. The number of folds $V$ can be set with the \code{folds} argument.  If data has a hierarchical structure, the \code{id} argument is used to indicate the name of a variable in the data set indicating unique groups. These identifiers will be used for generation of cross-validation folds and will be accounted for in standard error calculations. If a continuous outcome has known bounds, these bounds may be specified using the \code{bounds} argument with a length two numeric vector where the first index is the lower bound and the second index is the upper bound.

\hypertarget{examples}{\section{Examples}\label{examples}}

\hypertarget{example-1-longitudinal-mtp-with-no-loss-to-follow-up}{
\begin{example}{Longitudinal MTP with no loss-to-follow-up}
\end{example}}

We have simulated data on \(n = 5000\) observations over a 5-month period. Each observation has a continuous exposure (\code{A\_1}, \code{A\_2}, \code{A\_3}, \code{A\_4}) and covariate (\code{L\_1}, \code{L\_2}, \code{L\_3}, \code{L\_4}) recorded at months one through four and a dichotomous outcome (\code{Y}) at month five. We assume no loss-to-follow-up and no Markov assumption. This data set is installed with the package and is stored in the object \code{sim\_t4}.

For this example, we are interested in the effect of a longitudinal MTP where at each month an observation's exposure decreases by one only if their observed exposure wouldn't be less than one if modified. Our data structure has no baseline confounders and we will use only GLMs for estimation so the only objects we must specify are the treatment variables, the time-varying covariates, the outcome variable, and the MTP shift function.

\begin{CodeChunk}
\begin{CodeInput}
R> A <- c("A_1", "A_2", "A_3", "A_4")
R> L <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
R> 
R> shift <- function(data, trt) {
R+   (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) + 
R+     data[[trt]] * (data[[trt]] - 1 < 1)
R+ }
R> 
R> lmtp_tmle(sim_t4, A, "Y", time_vary = L, shift = f, 
R+           intervention_type = "mtp", folds = 1)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: TMLE
   Trt. Policy: (f)

Population intervention effect
      Estimate: 0.2524
    Std. error: 0.0104
        95% CI: (0.2319, 0.2728)
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{example-2-longitudinal-mtp-right-censoring}{
\begin{example}{Longitudinal MTP with right censoring}
\end{example}}

For this example, we have a simulated dataset of \(n = 1000\) observations. Data was simulated for three time points with a continuous time-varying exposure at times \(t \in \{1, 2\}\) (\code{A1}, \code{A2}), a dichotomous time-varying covariate at times \(t \in \{1, 2\}\) (\code{L1}, \code{L2}), and a dichotomous outcome (\code{Y}) at time \(\tau + 1 = 3\). Loss-to-follow-up is present after time \(t = 1\) so the data set contains censoring indicators (\code{C1}, \code{C2}). This data is installed with the package and is stored in the object \code{sim\_cens}.

Suppose we are interested in the additive effect of an MTP where exposure is increased by 0.5 at every time point for all observations. Instead of using a linear model,  we will estimate the outcome regression and treatment mechanism using a super learner composed of a generalized linear model, a random forest \citep{wrightRanger}, and multivariate adaptive regression splines \citep{milborrowEarth}. 

\begin{CodeChunk}
\begin{CodeInput}
R> A <- c("A1", "A2")
R> C <- c("C1", "C2")
R> L <- list(c("L1"), c("L2"))
R+
R> f <- function(data, trt) data[[trt]] + 0.5
R> sl_lib <- c("SL.glm", "SL.ranger", "SL.earth")
R+
R> lmtp_sdr(sim_cens, A, "Y", time_vary = L, cens = C,
R+         shift = f, intervention_type = "mtp", folds = 1,
R+         learners_trt = sl_lib, learners_outcome = sl_lib)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR   Trt. Policy: (f)Population intervention effect      Estimate: 0.9011    Std. error: 0.0102        95% CI: (0.8811, 0.9212)
\end{CodeOutput}
\end{CodeChunk}

If loss-to-follow-up exists, we can estimate the population mean outcome under the observed exposures adjusting for informative loss-to-follow-up by specifying \code{shift = NULL}. 

\begin{CodeChunk}

\begin{CodeInput}
R> lmtp_sdr(sim_cens, A, "Y", time_vary = L, cens = C,
R+          shift = NULL, folds = 1, learners_trt = sl_lib, 
R+          learners_outcome = sl_lib)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR   Trt. Policy: (NULL)Population intervention effect      Estimate: 0.7978    Std. error: 0.0128        95% CI: (0.7726, 0.8229)
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{example-3-survival-analysis-and-deterministic-effects}{
\begin{example}{Survival analysis and deterministic effects}
\end{example}}

The \pkg{lmtp} package may also be used to estimate deterministic causal effects, such as the causal relative risk. Suppose we have time-to-event data on \(n = 2000\) observations with a time-invariant dichotomous exposure followed for a period of seven days. We wish to estimate the causal relative risk of experiencing the event by day seven. This data is installed with the package and is stored in the object \code{sim\_point\_surv}. 

\begin{CodeChunk}
\begin{CodeInput}
R> A <- "trt"
R> W <- c("W1", "W2")
R> C <- paste0("C.", 0:5)
R> Y <- paste0("Y.", 1:6)
R>
R> a_1 <- lmtp_tmle(sim_point_surv, A, Y, W, cens = C,
R+                  outcome_type = "survival", folds = 1,
R+                  learners_trt = sl_lib,
R+                  learners_outcome = sl_lib,
R+                  shift = static_binary_on)
R> 
R> a_0 <- lmtp_tmle(sim_point_surv, A, Y, W, cens = C,
R+                  outcome_type = "survival", folds = 1,
R+                  learners_trt = sl_lib,
R+                  learners_outcome = sl_lib,
R+                  shift = static_binary_off)
R>                  
R> lmtp_contrast(a_1, ref = a_0, type = "rr")
\end{CodeInput}

\begin{CodeOutput}
  LMTP Contrast: relative riskNull hypothesis: theta == 1  theta shift   ref std.error conf.low conf.high p.value1 0.558 0.187 0.335    0.0382    0.518     0.601  <0.001
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{example-4-dynamic-treatment-regimes}{
\begin{example}{Dynamic treatment regimes}
\end{example}}

Dynamic treatment regimes are treatment rules where treatment is applied based on a fixed rule that depends on covariate history. The \pkg{lmtp} package is capable of estimating the effects of deterministic dynamic treatment rules and modified treatment policies that depend on covariate history. 

For this example we will use the same data from example 1. However, we will extend the longitudinal MTP used in that example so that a shift in the exposure also depends on covariate history and time. Specifically, if $t = 1$ exposure decreases by one if an observation's realized exposure wouldn’t be less than one if modified. If $t > 1$ and the value of the time-varying covariate at that time-point equals one then exposure should decrease by one if an observation's realized exposure wouldn’t be less than one if modified; otherwise exposure shouldn't be modified.

\begin{CodeChunk}
\begin{CodeInput}
R> A <- c("A_1", "A_2", "A_3", "A_4")
R> L <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
R> 
R> f <- function(data, trt) {
R+   mtp <- function(data, trt) {
R+     (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) +
R+       data[[trt]] * (data[[trt]] - 1 < 1)
R+   }
R+ 
R+   if (trt == "A_1") {
R+     return(mtp(data, trt))
R+   }
R+ 
R+   ifelse(
R+     data[[sub("A", "L", trt)]] == 1,
R+     mtp(data, trt),
R+     data[[trt]]
R+   )
R+ } 
R>
R> lmtp_sdr(sim_t4, A, "Y", time_vary = L, shift = f,
R>          intervention_type = "mtp", folds = 1,
R>          learners_outcome = sl_lib, learners_trt = sl_lib)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR   Trt. Policy: (f)Population intervention effect      Estimate: 0.328    Std. error: 0.0069        95% CI: (0.3144, 0.3416)
\end{CodeOutput} 
\end{CodeChunk}

\hypertarget{summary}{\section{Summary}\label{summary}}

The \pkg{lmtp} from a general framework for estimating the causal effects of binary, categorical, and continuous exposures from observational data while leveraging machine learning. 

\newpage

\bibliography{lmtp.bib}

\end{document}
